par(mar=c(5.1,4.1,4.1,2.1)/2)
par(mfrow=c(2,2))
acf(ndvi.array[long_index[1],lat_index[3],1,])
#ccf: Auto- and Cross- Covariance and -Correlation Function Estimation
#cross-correlogram (related to autocorrelations?)
#covariance between two time series, with different lags
par(mar=c(5.1,4.1,4.1,2.1)/1)
par(mfrow=c(3,1))
ccf(ndvi.array[long_index[1],lat_index[3],1,], ndvi.array[long_index[8],lat_index[3],1,]) #seasonal pattern
ccf(ndvi.array[long_index[8],lat_index[3],1,], ndvi.array[long_index[15],lat_index[3],1,]) #seasonal pattern
ccf(ndvi.array[long_index[15],lat_index[3],1,], ndvi.array[long_index[1],lat_index[3],1,]) #seasonal pattern
#different scales
par(mar=c(5.1,4.1,4.1,2.1)/1)
par(mfrow=c(3,1))
#plot(ndvi.array[long_index[8],lat_index[3],1,], ylim=c(-2,2), type='o')
#plot(ndvi.array[long_index[8],lat_index[3],2,], ylim=c(-2,2), type='o')
#plot(ndvi.array[long_index[8],lat_index[3],3,], ylim=c(-2,2), type='o')
#(a) between 1-month and 3-months
ccf(ndvi.array[long_index[8],lat_index[3],1,-c(1,2)], ndvi.array[long_index[8],lat_index[3],2,-c(1,2)]) #seasonal pattern, but slightly different from the above
#(b) between 1-month and 12-months: correlated each other?
ccf(ndvi.array[long_index[8],lat_index[3],1,-c(1:11)], ndvi.array[long_index[8],lat_index[3],3,-c(1:11)]) #seasonal pattern, but slightly different from the above
#(c) between 3-months and 12-months: correlated each other?
ccf(ndvi.array[long_index[8],lat_index[3],2,-c(1:11)], ndvi.array[long_index[8],lat_index[3],3,-c(1:11)]) #seasonal pattern, but slightly different from the above
#time values (from 1900 to 2099)
ss <- nc_data$dim$time$vals
as.POSIXct(ss*3600,origin='1850-01-01 00:00:00') #date conversion
########################################
#Step 1: marginal distribution modeling
########################################
#according to Ben Alaya (2018), to avoid the declustering steps, and subjectivity steps related to the application of the POT method,
#parameters of the GPD distribution and the threshold are estimated using annual maxima of X_i
#taking into account the duality between the two asymptotic GEV and GPD distributions.
########(compute annual maxima from the data)########
#ndvi.array.annual: annual maxima
init.computation <- TRUE
u.const.computation <- TRUE
quantVec <- c(70)/100
library(VGAM)
ndvi_max_aggregation <- TRUE
if(ndvi_max_aggregation==TRUE){
aggregation_time <- 3
ndvi_max_annual <- array(0, dim=c(dim(ndvi.array)[1], dim(ndvi.array)[2], dim(ndvi.array)[3], dim(ndvi.array)[4]/aggregation_time), dimnames=list(nc_data$dim$longitude$vals, nc_data$dim$latitude$vals, c(1,3,12), c(1900:(1900 -1 + dim(ndvi.array)[4]/aggregation_time)) ))
CANGRD_max_annual <- array(0, dim=c(dim(CANGRD.array)[1], dim(CANGRD.array)[2], dim(CANGRD.array)[3], dim(CANGRD.array)[4]/aggregation_time), dimnames=list(CANGRD_data$dim$longitude$vals, CANGRD_data$dim$latitude$vals, c(1,3,12), c(1900:(1900 -1 + dim(CANGRD.array)[4]/aggregation_time)) ))
for(i in 1:dim(ndvi_max_annual)[1]){
for(j in 1:dim(ndvi_max_annual)[2]){
if(AllNA[i,j]!=0){
if(aggregation_time!=3){
#compute yearly maxima
for(l in 1:(dim(ndvi.array)[4]/aggregation_time)){
ndvi_max_annual[i,j,1,l] <- max(ndvi.array[i,j,1,c(((l-1)*aggregation_time+1):(l*aggregation_time))], na.rm=T)
ndvi_max_annual[i,j,2,l] <- max(ndvi.array[i,j,2,c(((l-1)*aggregation_time+1):(l*aggregation_time))], na.rm=T)
ndvi_max_annual[i,j,3,l] <- max(ndvi.array[i,j,3,c(((l-1)*aggregation_time+1):(l*aggregation_time))], na.rm=T)
}
for(m in 1:(dim(CANGRD.array)[4]/aggregation_time)){
CANGRD_max_annual[i,j,1,m] <- max(CANGRD.array[i,j,1,c(((m-1)*aggregation_time+1):(m*aggregation_time))], na.rm=T)
CANGRD_max_annual[i,j,2,m] <- max(CANGRD.array[i,j,2,c(((m-1)*aggregation_time+1):(m*aggregation_time))], na.rm=T)
CANGRD_max_annual[i,j,3,m] <- max(CANGRD.array[i,j,3,c(((m-1)*aggregation_time+1):(m*aggregation_time))], na.rm=T)
}
}else{
#compute yearly maxima
for(l in 1:((dim(ndvi.array)[4]/aggregation_time)-1)){
ndvi_max_annual[i,j,1,l] <- max(ndvi.array[i,j,1,c(((l-1)*aggregation_time+3):(l*aggregation_time+2))], na.rm=T)
ndvi_max_annual[i,j,2,l] <- max(ndvi.array[i,j,2,c(((l-1)*aggregation_time+3):(l*aggregation_time+2))], na.rm=T)
ndvi_max_annual[i,j,3,l] <- max(ndvi.array[i,j,3,c(((l-1)*aggregation_time+3):(l*aggregation_time+2))], na.rm=T)
}
for(m in 1:((dim(CANGRD.array)[4]/aggregation_time)-1)){
CANGRD_max_annual[i,j,1,m] <- max(CANGRD.array[i,j,1,c(((m-1)*aggregation_time+3):(m*aggregation_time+2))], na.rm=T)
CANGRD_max_annual[i,j,2,m] <- max(CANGRD.array[i,j,2,c(((m-1)*aggregation_time+3):(m*aggregation_time+2))], na.rm=T)
CANGRD_max_annual[i,j,3,m] <- max(CANGRD.array[i,j,3,c(((m-1)*aggregation_time+3):(m*aggregation_time+2))], na.rm=T)
}
}
}else if(AllNA[i,j]==0){
for(l in 1:(dim(ndvi.array)[4]/aggregation_time)){
ndvi_max_annual[i,j,1,l] <- NA
ndvi_max_annual[i,j,2,l] <- NA
ndvi_max_annual[i,j,3,l] <- NA
}
for(m in 1:(dim(CANGRD.array)[4]/aggregation_time)){
CANGRD_max_annual[i,j,1,m] <- NA
CANGRD_max_annual[i,j,2,m] <- NA
CANGRD_max_annual[i,j,3,m] <- NA
}
}
}
}
CANGRD_max_annual[which(CANGRD_max_annual==-Inf)] <- NA
ndvimaxannualdata <- list()
ndvimaxannualdata$ndvi_max_annual <- ndvi_max_annual
ndvimaxannualdata$CANGRD_max_annual <- CANGRD_max_annual
ndvimaxannualdata$AllNA <- AllNA
#saveRDS(ndvimaxannualdata, "~/Dropbox/Data/PCIC/Drought/SPEICMIP5/Seoncheol/ndvimaxannualdata.RDS")
}else{
ndvimaxannualdata<- readRDS("~/Dropbox/Data/PCIC/Drought/SPEICMIP5/Seoncheol/ndvimaxannualdata.RDS")
ndvi_max_annual <- ndvimaxannualdata$ndvi_max_annual
CANGRD_max_annual <- ndvimaxannualdata$CANGRD_max_annual
AllNA <- ndvimaxannualdata$AllNA
}
ndvi_max_annual_historical <- ndvi_max_annual[,,,c(1:(1*(4*106)))]
ndvi_max_annual_actual<- ndvi_max_annual[,,,c((1+4*50):(4*106))]
CANGRD_max_annual_historical <- CANGRD_max_annual[,,,c(1:(1*(4*106)))]
CANGRD_max_annual_actual<- CANGRD_max_annual[,,,c((1+4*50):(4*106))]
##ndvi.array.annual을 어떻게 정하느냐가 중요
ndvi.array.annual <- ndvi_max_annual_historical[,,,c(1:dim(ndvi_max_annual_historical)[4])%%4 %in% c(2,3)]
u.array.annual <- array(0, dim=c(dim(ndvi.array.annual)[1], dim(ndvi.array.annual)[2], dim(ndvi.array.annual)[3], dim(ndvi.array.annual)[4]),  dimnames=list(nc_data$dim$longitude$vals, nc_data$dim$latitude$vals, c(1,3,12), c(1900:(1900 -1 + dim(ndvi.array.annual)[4]  ))))
CANGRD.array.annual <- CANGRD_max_annual_historical[,,,c(1:dim(CANGRD_max_annual_historical)[4])%%4 %in% c(2,3)]
for(i in 1:dim(ndvi.array.annual)[1]){
for(j in 1:dim(ndvi.array.annual)[2]){
if(AllNA[i,j]!=0 & aggregation_time==3){
if(u.const.computation==TRUE){
u.array.annual[i,j,1,] <- quantile(ndvi.array.annual[i,j,1,], prob=quantVec, na.rm=T)
u.array.annual[i,j,2,] <- quantile(ndvi.array.annual[i,j,2,], prob=quantVec, na.rm=T)
u.array.annual[i,j,3,] <- quantile(ndvi.array.annual[i,j,3,], prob=quantVec, na.rm=T)
}else{
}
}
}
}
###
hist(ndvi_max_annual_historical[11,24,1,c(1:(4*106))%%4 %in% c(2,3)], breaks=10)
gpd.fit(ndvi_max_annual_historical[11,24,1,c(1:(4*106))%%4 %in% c(2,3)], threshold=quantile(ndvi_max_annual_historical[11,24,1,c(1:(4*106))%%4 %in% c(2,3)], prob=0.7, na.rm=T))
acf(ndvi_max_annual_historical[11,24,1,c(1:(4*106))%%4 %in% c(2,3)])
###
###
hist(ndvi_max_annual_actual[11,24,1,c(1:224)%%4 %in% c(2,3)], breaks=10)
gpd.fit(ndvi_max_annual_actual[11,24,1,c(1:224)%%4 %in% c(2,3)], threshold=quantile(ndvi_max_annual_actual[11,24,1,c(1:224)%%4 %in% c(2,3)], prob=0.7, na.rm=T))
acf(ndvi_max_annual_actual[11,24,1,c(1:224)%%4 %in% c(2,3)])
###
#stationary test
plot(ndvi.array.annual[long_index[8],lat_index[3],1,], ylab="SPEI", main="(a) Original SPEI") #nonstationary
lines(u.array.annual[long_index[8],lat_index[3],1,], col="red")
acf(ndvi.array.annual[long_index[8],lat_index[3],1,], main="(b) SPEI")
plot(ndvi.array.annual[long_index[8],lat_index[3],1,]-u.array.annual[long_index[8],lat_index[3],1,], ylab="SPEI-u", main="(c) Residual SPEI") #nonstationary
abline(h=0, col="red")
acf(ndvi.array.annual[long_index[8],lat_index[3],1,]-u.array.annual[long_index[8],lat_index[3],1,], main="(d) SPEI-u")
#Ljung-Box test for independence -> H1: nonstationary
lag.length <- c(1:12)
Box.test.result <- list()
for(i in lag.length){
#Box.test.result[[i]] <- Box.test(ndvi.array.annual[long_index[8],lat_index[3],1,], lag=lag.length[i], type="Ljung-Box")
Box.test.result[[i]] <- Box.test(ndvi.array.annual[long_index[8],lat_index[3],1,]-u.array.annual[long_index[8],lat_index[3],1,], lag=lag.length[i], type="Ljung-Box")
}
#ADF test
options(warn=-1)
library(tseries)
adf.test(ndvi.array.annual[long_index[8],lat_index[3],1,])
adf.test(ndvi.array.annual[long_index[8],lat_index[3],1,]-u.array.annual[long_index[8],lat_index[3],1,])
#Kwiatkowski-Phillips-Schmidt-Shin (KPSS) for level or trend (H0) stationarity
kpss.test(ndvi.array.annual[long_index[8],lat_index[3],1,], null="Trend")
kpss.test(ndvi.array.annual[long_index[8],lat_index[3],1,]-u.array.annual[long_index[8],lat_index[3],1,], null="Trend")
##END
#####(여기서부터 살펴보기, ex.geofd 해보기)
## select ndvi.array.flat: there are various choices
##goal of ndvi.array.flat: to make two-dimensional data
##size of ndvi.array.flat: (length of year)*(long*lat) (ex. 49*60)
#not working
longlat_index <- expand.grid(long_index, lat_index)
#ndvi.array.candidate <- ndvi.array.historical[longlat_index[,1], longlat_index[,2], 1, -c(1:(12*50))]
ndvi.array.candidate <- ndvi.array.annual[longlat_index[,1], longlat_index[,2], 1, ] - u.array.annual[longlat_index[,1], longlat_index[,2], 1, ]
ndvi.array.flat <- data.frame(matrix(nrow=dim(ndvi.array.candidate)[3], ncol=nrow(longlat_index)))
for(i in 1:nrow(longlat_index)){
#ndvi.array.flat[,i] <- ndvi.array.historical[longlat_index[i,1], longlat_index[i,2], 1, -c(1:(12*50))]
ndvi.array.flat[,i] <- ndvi.array.annual[longlat_index[i,1], longlat_index[i,2], 1, ]
colnames(ndvi.array.flat)[i] <- paste("X", nc_data$dim$longitude$vals[longlat_index[i,1]], nc_data$dim$latitude$vals[longlat_index[i,2]], sep=".")
}
ndvi.array.flat <- ndvi.array.flat#[-nrow(ndvi.array.flat),]
#pdf("Stationcorrelation.pdf", 6, 6)
#png("Stationcorrelation.png", 600, 600)
GGally::ggpairs(ndvi.array.flat[,c(35:41)])
#dev.off()
##Exploring pairwise extremal dependence
chi03 <- chi(ndvi.array.flat[,c("X.-115.51", "X.-108.51")]) #limit goes to 1: stronger dependence?
ggplot(chi03, main=c("Chi: -115.51 and -108.51", "Chibar: -115.51 and -108.51"))
#(April, 15) chi-plot shows that extreme between (-115, 51) and (-108, 51) is independent
chi03 <- chi(ndvi.array.flat[,c("X.-115.51", "X.-114.51")]) #limit goes to 1: stronger dependence?
ggplot(chi03, main=c("Chi: -115.51 and -114.51", "Chibar: -115.51 and -114.51"))
#(April, 15) chi-plot shows that extreme between (-115, 51) and (-114, 51) is somewhat dependent
#MCS: multivariate conditional Spearman's correlation coeffs (in texmex)
mcs112 <- MCS(ndvi.array.flat[,c("X.-115.51", "X.-108.51")]) #positive assoc. between two variables
g1 <- ggplot(mcs112, main="MCS: -115.51 and -108.51")
#If you want to plot bootstrapted interval, (in texmex)
bootMCS112  <- bootMCS(ndvi.array.flat[,c("X.-115.51", "X.-108.51")])
g2 <- ggplot(bootMCS112, main="MCS: -115.51 and -108.51") #MCS with bootstrap CI
gridExtra::grid.arrange(g1, g2, ncol=1)
#end of eda
#(Apr 24) migpd version으로 작성
#migpd_method <- c("GEV", "GPD")
migpd_method <- c("GPD", "GEV")
use_evm <- TRUE
use_constant_threshold <- FALSE; dqu=0.7
ndvi.array.choice <- ndvi.array.annual#.historical
u.array.choice <- u.array.annual
CANGRD.array.choice <- CANGRD.array.annual#.historical
########################################
#Step 3-(a) 자료의 특성에 따라서 우리가 원하는 longlat_index, long_index, lat_index, 그리고 xy_grid를 정의하자
########################################
study_region_index <- c("(a)All", "(b)Palliser", "(c)BC", "(d)Central", "(e)East", "(f)Northern")
study_region <- study_region_index[5]
subsetting <- TRUE
#If you want to do use an expanded grid set,
expanded_griddata <- readRDS("~/Dropbox/R files/PCIC/Heffernan/sources/gridcity.RDS")
if(study_region=="(a)All"){
longlat_index <- which(AllNA3!=0, arr.ind=TRUE)
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
}else if(study_region=="(b)Palliser"){
#according to WIKI
long_index <- which(nc_data$dim$longitude$vals>=-115 & nc_data$dim$longitude$vals<=-101) #longitude
lat_index <- which(nc_data$dim$latitude$vals>=50 & nc_data$dim$latitude$vals<=53) #latitude
longlat_index <- expand.grid(long_index, lat_index)
xy_grid <- expand.grid(nc_data$dim$longitude$vals[long_index], nc_data$dim$latitude$vals[lat_index])
}else if(study_region=="(c)BC"){
#AllNA3==2
longlat_index <- which(AllNA3==2, arr.ind=TRUE)
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
}else if(study_region=="(d)Central"){
#AllNA3==1
longlat_index <- which(AllNA3==1, arr.ind=TRUE)
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
}else if(study_region=="(e)East"){
#AllNA3==4
longlat_index <- which(AllNA3==4, arr.ind=TRUE)
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
}else if(study_region=="(f)Northern"){
#AllNA3==3
longlat_index <- which(AllNA3==3, arr.ind=TRUE)
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
}
#켈로나, 사스카툰, 세그니
city <- c("None") #c("Saguenay") #c("Saskatoon") #c("Kelowna")
#city <- c("Kelowna")
#city <- c("Saskatoon")
city <- c("Saguenay")
if(study_region=="(c)BC" & city=="Kelowna"){
loc_Kelowna <- expanded_griddata$city[15,] #켈로나
central_index <- which.min(apply(as.matrix(xy_grid), 1, function(x) sqrt( (x[1]-as.numeric(loc_Kelowna[6]))^2 + (x[2]-as.numeric(loc_Kelowna[5]))^2)))
}else{
central_index <- which(apply(longlat_index, 1, function(x) all(round(apply(longlat_index, 2, median))==x)))
}
plot.divisor <- 1
if(subsetting==TRUE & !(study_region %in% c("(b)Palliser", "(c)BC"))){
#coaser scale
divisor <- sum(longlat_index[central_index,])%%2
subset_index <- which(apply(longlat_index, 1, function(x) sum(x)%%2) == divisor)
longlat_index <- longlat_index[subset_index,]
long_index <- unique(longlat_index[,1])
lat_index <- unique(longlat_index[,2])
xy_grid <- data.frame(Var1=nc_data$dim$longitude$vals[longlat_index[,1]], Var2=nc_data$dim$latitude$vals[longlat_index[,2]])
central_index <- which(apply(longlat_index, 1, function(x) all(round(apply(longlat_index, 2, median))==x)))
plot.divisor <- 2
}
if(study_region=="(d)Central" & city=="Saskatoon"){
loc_Saskatoon <- expanded_griddata$city[12,] #사스카툰
central_index <- which.min(apply(as.matrix(xy_grid), 1, function(x) sqrt( (x[1]-as.numeric(loc_Saskatoon[6]))^2 + (x[2]-as.numeric(loc_Saskatoon[5]))^2)))
}else if(study_region=="(e)East" & city=="Saguenay"){
loc_Saguenay <- expanded_griddata$city[14,] #세그니
central_index <- which.min(apply(as.matrix(xy_grid), 1, function(x) sqrt( (x[1]-as.numeric(loc_Saguenay[6]))^2 + (x[2]-as.numeric(loc_Saguenay[5]))^2)))
}
result_dataset <- c()
CANGRD_dataset <- c()
for(i in 1:nrow(xy_grid)){
x_ind <- match(xy_grid[i,1], as.numeric(dimnames(ndvi.array.annual)[[1]]))
y_ind <- match(xy_grid[i,2], as.numeric(dimnames(ndvi.array.annual)[[2]]))
new_col <- matrix(ndvi.array.choice[x_ind,y_ind,1,], ncol=1)
colnames(new_col)[1] <- paste(xy_grid[i,1], xy_grid[i,2], sep="=")
result_dataset <- cbind(result_dataset, new_col)
CANGRD_col <- matrix(CANGRD.array.choice[x_ind,y_ind,1,], ncol=1)
colnames(CANGRD_col)[1] <- paste(xy_grid[i,1], xy_grid[i,2], sep="=")
CANGRD_dataset <- cbind(CANGRD_dataset, CANGRD_col)
}
central_name <- colnames(result_dataset)[central_index]
########################################
#August 26, 2020: central_index 고르기: .9? return level을 구해서 가장 큰 쪽으로 하자
########################################
if(city!="None"){
change_central=FALSE
}else{
change_central=TRUE
}
if(change_central==TRUE){
dist_min <- min(dist(xy_grid))
central_cand <- which(apply(as.matrix(dist(xy_grid)), 1, function(x) sum(x==dist_min))==4)
#paste(dimnames(ndvi.array.annual)[[1]][43], dimnames(ndvi.array.annual)[[2]][12], sep="=") "-108=51"
compute_gpdRl <- function(x){
fit <- eva::gpdFit(as.numeric(x), threshold=quantile(as.numeric(x), prob=0.7))
gpdRl(fit, period=10, method="delta")$Estimate
}
#central_ind_cand <- strsplit(names(which.max(apply(result_dataset, 2, compute_gpdRl))), split="=")
central_index <- intersect(order(apply(result_dataset, 2, compute_gpdRl), decreasing=TRUE), central_cand)[1]
}
central_name <- colnames(result_dataset)[central_index]
########################################
#(끝) August 26, 2020: central_index 고르기: .9? return level을 구해서 가장 큰 쪽으로 하자
########################################
migpd_obj <- migpd(result_dataset, mqu=0.7)
mexDep_imsi <- mexDependence(x=migpd_obj, which=central_name , dqu=0.7)
CANGRD_obj <- migpd(CANGRD_dataset[-c(1:98),], mth=apply(CANGRD_dataset, 2, quantile, prob=0.7, na.rm=T))
CANGRD_mexDep_imsi <- mexDependence(x=migpd_obj, which=central_name , dqu=0.7)
#boot_imsi <- bootmex(mexDep_imsi)
########################################
#Step 3-(b) when there are more than one unconditional variables (two, three, ...)
########################################
#xy_index_conditional <- c(long_index[5], lat_index, 1)
#xy_index_unconditional <- cbind(long_index[-5], lat_index, 1)
xy_index_conditional <- c(longlat_index[central_index,1], longlat_index[central_index,2], 1)
xy_index_unconditional <- cbind(longlat_index[-central_index,1], longlat_index[-central_index,2], 1)
u <- u.array.choice[xy_index_conditional[1], xy_index_conditional[2], xy_index_conditional[3],]
##data
Y <- mexDep_imsi$margins$transformed
#data_likelihoo의 Inf 처리 필요
#extreme_index <- which(Y[xy_index_conditional[1], xy_index_conditional[2], xy_index_conditional[3],] > u) #approximately 120
extreme_index <- which(Y[,central_index] > u) #approximately 120
data_likelihood <- data.frame(X00=Y[extreme_index, central_index])
for(i in 1:nrow(longlat_index)){
if(i!=central_index){
data_frame_imsi <- data.frame(Y[extreme_index,i])
names(data_frame_imsi) <- paste("X1", i, sep="")
data_likelihood <- cbind(data_likelihood, data_frame_imsi)
#data_likelihood <- cbind(data_likelihood, paste("X1", i, sep="")=Y[xy_index_unconditional[i,1], xy_index_unconditional[i,2], xy_index_unconditional[i,3],extreme_index])
}
}
data <- data_likelihood #for the test purpose
library(Kendall)
Kendall_tau <- c()
for(i in 1:(ncol(data)-1)){
Kendall_tau <- c(Kendall_tau, Kendall(data$X00, data[,(i+1)])$tau[1])
}
par(mfrow=c(2,2))
Kendall(data$X00, data$X11)
hist(data$X11)
hist(data$X12)
hist(data$X13)
hist(data$X14)
########################################
#Step 4: a(h), b(h), mu(h), sigma(h) estimation step
########################################
nonlinear_estimation_obj <- nonlinear_estimation(mexDepobj=mexDep_imsi, xy_grid=xy_grid)
#mexDepobj=mexDep_imsi
########################################
#Step 5: estimation
########################################
##initial values
margins_L <- list( p2q = function(p)ifelse(p < .5, log(2 * p), -log(2 * (1 - p))),
q2p = function(q)ifelse(q < 0, exp(q)/2, 1- 0.5*exp(-q)))
#(SC): there is an R cod mexTransform.R, default method: mixture
#(Apr 17) 아마도 Gumbel 또는 Laplace 마진에 맞추어 x(data)를 바꾸는 함수인듯
x_tr <- mexTransform(migpd_obj, margins = margins_L, method = "mixture", r=NULL) #(SC) marginal transform!
dqu <- 0.7
dth <- quantile(x_tr$transformed[, central_index], dqu) #(SC) if dqu=0.7, it computes 70% quatile
yex <- c(x_tr$transformed[, central_index])
wh <- yex > unique(dth)
#additionally,
#phi in (1,2)
ne <- nonlinear_estimation_obj
ev <- 0.0001
#init.lower <- as.numeric(c(nonlinear_estimation_obj$pars-ev, 0.1, sqrt(2), 0.5,1,0.901,0.01))
#init.upper <- as.numeric(c(nonlinear_estimation_obj$pars+ev, 2, 2, 0.7,2,1.124,2*pi))
init.lower <- as.numeric(c(nonlinear_estimation_obj$pars-ev, min(dist(xy_grid))/5, 1, 0.5,1,0.5,-1))
init.upper <- as.numeric(c(nonlinear_estimation_obj$pars+ev, max(dist(xy_grid))/5, 2, 0.7,2,2,1))
par.names <- c(names(nonlinear_estimation_obj$pars), "rho", "phi", "delta1", "delta2", "Stretch", "Angle")
par.lower <- as.numeric(c(rep(0.0001, length(nonlinear_estimation_obj$pars)), min(dist(xy_grid))/5, 1, 0.01, 0,0.5,-1))
par.upper <- as.numeric(c(rep(Inf, length(nonlinear_estimation_obj$pars)), max(dist(xy_grid))/5, 2, Inf, Inf,2,1))
if("sigm"%in%par.names){
par.lower[which(par.names=="sigm")] <- 0.2
}
known.par <- rep(NA, length(par.names))#; known.par[1] <- 1
#known.par <-  c(ne$kapp, ne$lambda, ne$bet, ne$bet_KS1, ne$bet_KS2, NA, NA, ne$sigm, ne$sigm_KS1, ne$sigm_KS2, ne$mu, ne$mu_KS1, ne$mu_KS2, ne$mu_KS3, NA, NA, NA ,NA)
aniso <- FALSE
aniso <- TRUE
global.min <- 0
tol <- 1e-2
time_init <- Sys.time()
#(8개를 넣는다면) 순서대로 "kapp", "lambda", "bet", "rho", "sigm", "mu", "delta1", "delta2"
#(9개를 넣는다면) 순서대로 "kapp", "lambda", "bet", "rho", "sigm_KS1", "sigm_KS2", "mu", "delta1", "delta2"
param_init2 <- GenSA(lower=init.lower, upper=init.upper,
fn= composite_likelihood_parallel,
par.names=par.names, known.par=known.par,
data=x_tr$transformed[wh,], xy_grid=xy_grid, spatialmethod="Shooter",
aniso=aniso, xy_grid_cond_index=central_index, modelbeta="SC", addsigma=FALSE,verbose=FALSE, nonlinear.obj=ne,
control=list(threshold.stop=global.min+tol, max.time=60, maxit=30))
param2 <- matrix(param_init2$par,nrow=1)
colnames(param2) <- par.names
composite_likelihood_parallel(par=param2, data=x_tr$transformed[wh,], xy_grid=xy_grid,par.names=par.names, known.par=known.par,
spatialmethod="Shooter", aniso=aniso, xy_grid_cond_index=central_index, modelbeta="SC", addsigma=FALSE,verbose=FALSE,
nonlinear.obj = ne)
mexDep_spatial_obj <- mexDependence_spatial(x=migpd_obj, which=central_name , dqu=0.7, xy_grid=xy_grid, start=param2,
par.lower=par.lower, par.upper=par.upper,
par.names=par.names, modelbeta="SC", addsigma=FALSE, verbose=FALSE, mexDep_HT=mexDep_imsi,
known.par=known.par, spatialmethod="Shooter", aniso=aniso, nonlinear.obj = ne, skip.opt=TRUE)
#x=migpd_obj; which=central_name; dqu=0.7; xy_grid; start=param2
#par.names=c("kapp", "lambda", "bet", "rho", "sigm", "mu", "delta1", "delta2"); known.par=c(NA,NA,NA,NA,NA,NA,NA,NA);  par.lower=c(0.01, 0.01, 0, 0, 0.5, -Inf, 0.01, -Inf); par.upper=c(Inf, Inf, 0.99, 0.99, Inf, Inf, Inf, Inf)
#spatialmethod="Shooter"; aniso
mexDep_spatial_obj
if(abs(mexDep_spatial_obj$dependence$loglik)>1000000){
mexDep_spatial_obj$dependence$coefficients <- as.matrix(param_init2$par, ncol=1)
rownames(mexDep_spatial_obj$dependence$coefficients) <- par.names
}
mexDep_spatial_obj$dependence$loglik
length(mexDep_spatial_obj$dependence$coefficients)
sum(mexDep_imsi$dependence$loglik)
## If you want to add more locations,
add.location <- FALSE
if(add.location==TRUE){
if(study_region=="(a)All"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$All==1),c(1:2)]
}else if(study_region=="(b)Palliser"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$Palliser==1),c(1:2)]
}else if(study_region=="(c)BC"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$BC==1),c(1:2)]
}else if(study_region=="(d)Central"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$Central==1),c(1:2)]
}else if(study_region=="(e)East"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$East==1),c(1:2)]
}else if(study_region=="(f)Northern"){
pred_additionalloc <- expanded_griddata$AllNA[which(expanded_griddata$AllNA$Northern==1),c(1:2)]
}
}
pred_additionalloc <- pred_additionalloc[!duplicated(pred_additionalloc),]
pred_additionalloc <- data.frame(Var1=pred_additionalloc[,1], Var2=pred_additionalloc[,2])
mexDep_spatial_obj_pred <- predict.mexspatial(mexDep_spatial_obj, pqu=0.71, maxpqu=0.99, predmodel="Shooter",
pred_additionalloc = pred_additionalloc)# default
## (END) If you want to add more locations,
#mexDep_spatial_obj_pred <- predict.mexspatial(mexDep_spatial_obj, smoothZdistribution=TRUE)
mexDep_spatial_obj_pred <- predict.mexspatial(mexDep_spatial_obj, pqu=0.71, maxpqu=0.99, predmodel="Shooter")# default
#object <- mexDep_spatial_obj; pqu=0.79; maxpqu=0.81; predmodel="Shooter"; throwdist="unif"; nsim = 1000; smoothZdistribution=FALSE
mexDep_spatial_obj_pred_8 <- predict.mexspatial(mexDep_spatial_obj, pqu=0.79, maxpqu=0.81, predmodel="Shooter")# default
mexDep_spatial_obj_pred_9 <- predict.mexspatial(mexDep_spatial_obj, pqu=0.89, maxpqu=0.91, predmodel="Shooter")# default
mexDep_spatial_obj_pred_95 <- predict.mexspatial(mexDep_spatial_obj, pqu=0.94, maxpqu=0.96, predmodel="Shooter")# defaultmexDep_spatial_obj_pred_95 <- predict.mexspatial(mexDep_spatial_obj, pqu=0.94, maxpqu=0.96)# default
mexDep_spatial_obj_pred_99 <- predict.mexspatial(mexDep_spatial_obj, pqu=0.985, maxpqu=0.995, predmodel="Shooter")# default
pred_val_8 <- apply(mexDep_spatial_obj_pred_8$data$simulated, 2, mean)
pred_val_9 <- apply(mexDep_spatial_obj_pred_9$data$simulated, 2, mean)
pred_val_95 <- apply(mexDep_spatial_obj_pred_95$data$simulated, 2, mean)
pred_val_99 <- apply(mexDep_spatial_obj_pred_99$data$simulated, 2, mean)
pred_val_8 <- pred_val_8[match(colnames(mexDep_spatial_obj$margins$data), names(pred_val_8))]
pred_val_9 <- pred_val_9[match(colnames(mexDep_spatial_obj$margins$data), names(pred_val_9))]
pred_val_95 <- pred_val_95[match(colnames(mexDep_spatial_obj$margins$data), names(pred_val_95))]
pred_val_99 <- pred_val_99[match(colnames(mexDep_spatial_obj$margins$data), names(pred_val_99))]
xy_expand <- expand.grid(nc_data$dim$longitude$vals, nc_data$dim$latitude$vals)
setwd("~/Dropbox/앱/ShareLaTeX/(Park)EDrought/images")
#pdf("Centralnonlinear.pdf", 8, 5)
#png("Centralnonlinear.png", 800, 500)
par(mar=c(4.1,5.1,3.1,3.1))
par(mfrow=c(2,2))
quilt.plot(xy_expand, as.vector(AllNA), nx=101, ny=46, add.legend = FALSE, xlab="Lon", ylab="Lat",col = gray.colors(3), main="(a) Nonlinear, 0.8q")
quilt.plot(xy_grid[,1], xy_grid[,2], pred_val_8, add=T, nx=length(unique(xy_grid[,1]))/plot.divisor, ny=length(unique(xy_grid[,2]))/plot.divisor)
points(xy_grid[central_index,1], xy_grid[central_index,2], pch=0)
quilt.plot(xy_expand, as.vector(AllNA), nx=101, ny=46, add.legend = FALSE, xlab="Lon", ylab="Lat",col = gray.colors(3), main="(b) Nonlinear, 0.9q")
quilt.plot(xy_grid[,1], xy_grid[,2], pred_val_9, add=T, nx=length(unique(xy_grid[,1]))/plot.divisor, ny=length(unique(xy_grid[,2]))/plot.divisor)
quilt.plot(xy_expand, as.vector(AllNA), nx=101, ny=46, add.legend = FALSE, xlab="Lon", ylab="Lat",col = gray.colors(3), main="(c) Nonlinear, 0.95q")
quilt.plot(xy_grid[,1], xy_grid[,2], pred_val_95, add=T, nx=length(unique(xy_grid[,1]))/plot.divisor, ny=length(unique(xy_grid[,2]))/plot.divisor)
quilt.plot(xy_expand, as.vector(AllNA), nx=101, ny=46, add.legend = FALSE, xlab="Lon", ylab="Lat",col = gray.colors(3), main="(d) Nonlinear, 0.99q")
quilt.plot(xy_grid[,1], xy_grid[,2], pred_val_99, add=T, nx=length(unique(xy_grid[,1]))/plot.divisor, ny=length(unique(xy_grid[,2]))/plot.divisor)
bootmex_spatial_obj <- readRDS("~/Dropbox/R files/PCIC/Heffernan/BootCI/(c)BC/(c)BC_Summer-Autumn_CESM1_7_6_001-005_bootmex.RDS")
bootmex_spatial_obj
bootmex_spatial_obj[[1]]
setwd("~/Dropbox/R files/PCIC/Heffernan/BootCI/(e)Eastern/")
files_candidate <- list.files()
bootmex_spatial_obj_list <- list()
for(i in 1:length(files_candidate)){
aaaa <- readRDS(files_candidate[i])
bootmex_spatial_obj_list <- c(bootmex_spatial_obj_list, aaaa$boot)
}
plot_bootmex_supp <- function(h, bootmexobj, names_candidate=NULL){
if(is.null(names_candidate)){
names_candidate <- c("kapp", "lambda", "alpha", "alpha_KS3", "bet", "bet_KS1", "bet_KS2", "rho", "phi", "sigm", "sigm_KS1", "sigm_KS2", "mu", "mu_KS1", "mu_KS2", "mu_KS3", "delta1", "delta2", "Stretch", "Angle")
}
for(i in 1:length(rownames(bootmexobj$dependence))){
assign(rownames(bootmexobj$dependence)[i], bootmexobj$dependence[i,1])
}
other_names <- setdiff(names_candidate, rownames(bootmexobj$dependence))
if(length(other_names)!=0){
for(j in 1:length(other_names)){
assign(other_names[j], NULL)
}
}
#alpha_result_vec <- c()
if(is.null(alpha)){
alpha <- 1
}
if(is.null(kapp)){
kapp <- 1
}
if(is.null(alpha_KS3)){
alpha_KS3 <- 0
}
if(!is.null(lambda) & !is.null(h)){
alpha_obj <- (alpha_KS3+ (alpha-alpha_KS3)*exp(-((h-0)/lambda)^kapp))
}else{
alpha_obj <- as.numeric(rep(alpha, length(h)))
}
plot(h, alpha_obj)
#bet_result_vec <- c()
if(is.null(bet)){
bet <- 1
}
if(is.null(bet_KS1)){
bet_KS1 <- 1
}
if(!is.null(bet_KS2) & !is.null(h)){
bet_obj <- (bet*exp(-((h-0)/bet_KS2)^(bet_KS1)))
}else{
bet_obj <- as.numeric(rep(bet, length(h)))
}
plot(h, bet_obj)
